# üé¨ Final Project: IMDB Data Analysis and Wikimedia Stream Processing

## üßë‚Äçü§ù‚Äçüßë Group Members
* AMADOU Kassim
* AZEMAR Solene
* BELET Marine
* BENABDELJALIL Yanis

## 1. Project Overview and Files

This project combines two main components:
1.  **IMDB Data Analysis (Static):** Handled in the Jupyter Notebook (`FinalProject.ipynb`).
2.  **Wikimedia Stream Processing (Real-Time):** Handled by the Python scripts (`wikimedia_producer.py` and `project.py`).

| File/Folder | Purpose | Status |
| :--- | :--- | :--- |
| `data/` | **[CRITICAL: MUST BE INCLUDED]** Contains all IMDB datasets (`*.tsv.gz`). | **Must be checked.** |
| `FinalProject.ipynb` | **Primary Deliverable.** Contains all IMDB analysis, answers in Markdown, and the final verification of streaming results. | **Completed.** |
| `wikimedia_producer.py` | **Python Producer:** Connects to the real Wikimedia EventStream and sends data to Kafka. | **Code Required.** |
| `project.py` | **Python Consumer:** Listens to Kafka, updates metrics, and generates alerts. | **Code Ready.** |
| `docker-compose.yml` | Kafka/Zookeeper environment setup. | **Config Ready.** |
| `requirements.txt` | Python dependencies (must include `kafka-python`, `pandas`, `sseclient`). | **Config Ready.** |
| `stream_metrics.json` | **[OUTPUT FILE]** Generated by `project.py` (must be included). | **Must be generated.** |
| `stream_alerts.log` | **[OUTPUT FILE]** Generated by `project.py` when the ALERT_USER is detected (must be included). | **Must be generated.** |

## 2. Full Execution Instructions (Three Terminals)

### Step 1: Setup and Environment

1.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
2.  **Start Docker Services (Kafka/Zookeeper):**
    ```bash
    docker-compose up -d
    ```
3.  **Create the Kafka Topic (`wikimedia.recentchange`):**
    ```bash
    docker exec -it kafka bash
    kafka-topics --create --topic wikimedia.recentchange --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
    exit
    ```

### Step 2: Start the Streaming Pipeline

Open **three separate terminals** in the project directory:

| Terminal | Command | Role |
| :--- | :--- | :--- |
| **Terminal A (Producer)** | `python wikimedia_producer.py` | Connects to the real Wikimedia stream and *pushes* data to your local Kafka broker. |
| **Terminal B (Consumer)** | `python project.py` | *Pulls* data from the Kafka broker, filters it, calculates metrics, and generates the output files (`*.json`, `*.log`). |
| **Terminal C (Jupyter)** | `jupyter notebook FinalProject.ipynb` | Used to run the static analysis and check the final streaming outputs. |

### Step 3: Verification and Output Generation

1.  **Wait 1-2 Minutes:** Allow the Producer (Terminal A) and Consumer (Terminal B) to run. The Consumer needs to process at least **100 events** to generate the final `stream_metrics.json`.
2.  **Check Terminal B:** Ensure you see logs confirming messages are being processed and that `Metrics saved...` appears.
3.  **Trigger the Alert:** If the alert hasn't been triggered naturally, you can manually send an alert message via a separate console producer (or ensure `project.py` is configured to log the event immediately).
    * *Note: For the submission, ensure **`stream_metrics.json`** and **`stream_alerts.log`** are present in the root folder.*

### Step 4: Final Analysis

1.  Open the **`FinalProject.ipynb`** in your browser.
2.  Run all cells to execute the IMDB analysis.
3.  Run the final code cell to load and display the data from **`stream_metrics.json`** and **`stream_alerts.log`**, concluding the project.